{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9-fI8n7nhwD"
   },
   "source": [
    "Ilaria Enache \n",
    "\n",
    "Computer Vision and Machine Learning Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc6XQihnxVW-"
   },
   "source": [
    "Link to the shared folder with all the data and datasets: https://drive.google.com/drive/folders/1caU3qpKqAhUKdmhJ0nLzWYzRu1ZAh1Y5?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnptLlHg_wj0"
   },
   "source": [
    "Importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3383,
     "status": "ok",
     "timestamp": 1668520414380,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "qiVnItyEv0qL"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "from google.colab.patches import cv2_imshow\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import random\n",
    "\n",
    "import io\n",
    "import cv2 # OpenCV library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppLNYA-DJ6xR"
   },
   "source": [
    "We start by testing if the camera is working and define the VideoCapture function to capture the images for the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1668520418719,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "dY5hPx29v9Lk",
    "outputId": "c596090f-4afb-4cab-b570-b429bfba704a"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    // the function 'create' creates the \"box\" that contains the videostream\n",
       "    // async functions return a promise, they make the code look synchronous, but it's asynchronous and non-blocking behind the scenes\n",
       "    async function create(){ \n",
       "      div = document.createElement('div');\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      video = document.createElement('video');\n",
       "      video.setAttribute('playsinline', '');\n",
       "\n",
       "      div.appendChild(video);\n",
       "      // await is used to call functions, the calling code will stop until the promise is resolved or rejected\n",
       "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
       "      video.srcObject = stream;\n",
       "\n",
       "      // await is used to call functions, here we call the video.play() function\n",
       "      await video.play();\n",
       "\n",
       "      canvas =  document.createElement('canvas');\n",
       "      canvas.width = video.videoWidth;\n",
       "      canvas.height = video.videoHeight;\n",
       "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "\n",
       "      div_out = document.createElement('div');\n",
       "      document.body.appendChild(div_out);\n",
       "      img = document.createElement('img');\n",
       "      div_out.appendChild(img);\n",
       "    }\n",
       "\n",
       "    //  \n",
       "    async function capture(){\n",
       "        return await new Promise(function(resolve, reject){\n",
       "            pendingResolve = resolve;\n",
       "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "            result = canvas.toDataURL('image/jpeg', 0.80);\n",
       "\n",
       "            pendingResolve(result);\n",
       "        })\n",
       "    }\n",
       "\n",
       "    function showimg(imgb64){\n",
       "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
       "    }\n",
       "\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VideoCapture creates a real time video stream \n",
    "\n",
    "def VideoCapture():\n",
    "  js = Javascript('''\n",
    "    // the function 'create' creates the \"box\" that contains the videostream\n",
    "    // async functions return a promise, they make the code look synchronous, but it's asynchronous and non-blocking behind the scenes\n",
    "    async function create(){ \n",
    "      div = document.createElement('div');\n",
    "      document.body.appendChild(div);\n",
    "\n",
    "      video = document.createElement('video');\n",
    "      video.setAttribute('playsinline', '');\n",
    "\n",
    "      div.appendChild(video);\n",
    "      // await is used to call functions, the calling code will stop until the promise is resolved or rejected\n",
    "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
    "      video.srcObject = stream;\n",
    "\n",
    "      // await is used to call functions, here we call the video.play() function\n",
    "      await video.play();\n",
    "\n",
    "      canvas =  document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "\n",
    "      div_out = document.createElement('div');\n",
    "      document.body.appendChild(div_out);\n",
    "      img = document.createElement('img');\n",
    "      div_out.appendChild(img);\n",
    "    }\n",
    "\n",
    "    //  \n",
    "    async function capture(){\n",
    "        return await new Promise(function(resolve, reject){\n",
    "            pendingResolve = resolve;\n",
    "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "            result = canvas.toDataURL('image/jpeg', 0.80);\n",
    "\n",
    "            pendingResolve(result);\n",
    "        })\n",
    "    }\n",
    "\n",
    "    function showimg(imgb64){\n",
    "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
    "    }\n",
    "\n",
    "  ''')\n",
    "  display(js)\n",
    "\n",
    "\n",
    "VideoCapture()\n",
    "eval_js('create()')\n",
    "\n",
    "image_max_width = 640\n",
    "image_max_height = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19437,
     "status": "ok",
     "timestamp": 1668520438153,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "xUx5166mwdYw",
    "outputId": "40026358-2754-47be-f87b-3180b3bcfade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#access google drive to use files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1668520438154,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "BkUIwhklhXFd"
   },
   "outputs": [],
   "source": [
    "# Given a variable byte containing the bytes of an image, returns an array representing the image\n",
    "def byte2image(byte):\n",
    "  jpeg = b64decode(byte.split(',')[1])\n",
    "  im = Image.open(io.BytesIO(jpeg))\n",
    "  return np.array(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1668520438154,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "KPKzJE5JhbNA"
   },
   "outputs": [],
   "source": [
    "#Given an array representing an image, returns the byte associated with the image\n",
    "def image2byte(image):\n",
    "  image = Image.fromarray(image)\n",
    "  buffer = io.BytesIO()\n",
    "  image.save(buffer, 'jpeg')\n",
    "  buffer.seek(0)\n",
    "  x = b64encode(buffer.read()).decode('utf-8')\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1668520438154,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "fNXv4eRUhgdd"
   },
   "outputs": [],
   "source": [
    "def detect(img, cascade):\n",
    "  # rects represent the coordinates of the rectangles containing different objects\n",
    "  rects = cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=4, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "  if len(rects) == 0: # if no rectangles are found, return the empty list\n",
    "    return []  \n",
    "  rects[:,2:] += rects[:,:2] #(x1,y1,x2,y2) we have the top left and bottom right corners of the rectangle\n",
    "\n",
    "  return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1668520438154,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "HuxiiONahnI0"
   },
   "outputs": [],
   "source": [
    "#draw the rects on the video\n",
    "def draw_rects(img, rects, color, size=2):\n",
    "  for x1, y1, x2, y2 in rects:\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlcqRbNlL0aq"
   },
   "source": [
    "# **Task 1** \n",
    "\n",
    "In this first task, I identified the face using a Haarcascade over a converted grayscale image. \n",
    "I used a gray image as they have less information to process, improving speed and efficiency compared to detection over a colored image. \n",
    "\n",
    "The region of interest is then identified and drawn with the coordinates identified in the detect function. \n",
    "It is important to note that the image is defined from the top left at 0 ,0 to the bottom right.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 30783,
     "status": "error",
     "timestamp": 1668520468924,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "hB4HjHqWtgQQ",
    "outputId": "476f0017-851d-47fc-88d1-c3e0642e93de"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    // the function 'create' creates the \"box\" that contains the videostream\n",
       "    // async functions return a promise, they make the code look synchronous, but it's asynchronous and non-blocking behind the scenes\n",
       "    async function create(){ \n",
       "      div = document.createElement('div');\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      video = document.createElement('video');\n",
       "      video.setAttribute('playsinline', '');\n",
       "\n",
       "      div.appendChild(video);\n",
       "      // await is used to call functions, the calling code will stop until the promise is resolved or rejected\n",
       "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
       "      video.srcObject = stream;\n",
       "\n",
       "      // await is used to call functions, here we call the video.play() function\n",
       "      await video.play();\n",
       "\n",
       "      canvas =  document.createElement('canvas');\n",
       "      canvas.width = video.videoWidth;\n",
       "      canvas.height = video.videoHeight;\n",
       "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "\n",
       "      div_out = document.createElement('div');\n",
       "      document.body.appendChild(div_out);\n",
       "      img = document.createElement('img');\n",
       "      div_out.appendChild(img);\n",
       "    }\n",
       "\n",
       "    //  \n",
       "    async function capture(){\n",
       "        return await new Promise(function(resolve, reject){\n",
       "            pendingResolve = resolve;\n",
       "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "            result = canvas.toDataURL('image/jpeg', 0.80);\n",
       "\n",
       "            pendingResolve(result);\n",
       "        })\n",
       "    }\n",
       "\n",
       "    function showimg(imgb64){\n",
       "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
       "    }\n",
       "\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-54b610acd1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0me3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTickCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showimg(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# detect face and eyes using the Haar Feature-based Cascade Classifiers\n",
    "\n",
    "VideoCapture() #starts the VideoCapture function\n",
    "eval_js('create()') #utilizes the given JS code\n",
    "\n",
    "image_max_width = 640\n",
    "image_max_height = 480\n",
    "\n",
    "#application of the Haarcascade for object detection faces\n",
    "CascadeFace = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "  e1 = cv2.getTickCount()\n",
    "\n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)  \n",
    "\n",
    "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # converting the image into GRAY reduces the color channels of the image to grayscale and improves computation speed \n",
    "  \n",
    "  face = detect(gray, cascade = CascadeFace) # # applies the Haar face cascade to detect the face int he gray image \n",
    "  draw_rects(im, face, (0, 0, 0)) # calls the draw function to draw a rectangle around the face\n",
    "\n",
    "  e2 = cv2.getTickCount()\n",
    "  face_time = e2-e1  # - CONVERT INTO SECONDS\n",
    "  #print(\"the number of clock-cycles to detect the face is :\", face_time)\n",
    "  e3 = cv2.getTickCount()\n",
    "\n",
    "  eval_js('showimg(\"{}\")'.format(image2byte(im)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oo2UU3WdL5R1"
   },
   "source": [
    "# **Task 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1668520472739,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "nG28ZofEyn9x"
   },
   "outputs": [],
   "source": [
    "#function to compute the bigger rectangle we are going to use as region of interest\n",
    "def compute_big_rect(rect): #[x1,y1,x2,y2]\n",
    "\n",
    "  rect[0]=max(0,rect[0]-margin)\n",
    "  rect[1]=max(0,rect[1]-margin)\n",
    "  rect[2]=min(rect[2]+margin,image_max_width)\n",
    "  rect[3]=min(rect[3]+margin,image_max_height)\n",
    "  \n",
    "  return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4272,
     "status": "error",
     "timestamp": 1668520478722,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "vjtsaSqtPXwU",
    "outputId": "dd0870e6-f37c-4698-a4d7-8549137f78b6"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    // the function 'create' creates the \"box\" that contains the videostream\n",
       "    // async functions return a promise, they make the code look synchronous, but it's asynchronous and non-blocking behind the scenes\n",
       "    async function create(){ \n",
       "      div = document.createElement('div');\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      video = document.createElement('video');\n",
       "      video.setAttribute('playsinline', '');\n",
       "\n",
       "      div.appendChild(video);\n",
       "      // await is used to call functions, the calling code will stop until the promise is resolved or rejected\n",
       "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
       "      video.srcObject = stream;\n",
       "\n",
       "      // await is used to call functions, here we call the video.play() function\n",
       "      await video.play();\n",
       "\n",
       "      canvas =  document.createElement('canvas');\n",
       "      canvas.width = video.videoWidth;\n",
       "      canvas.height = video.videoHeight;\n",
       "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "\n",
       "      div_out = document.createElement('div');\n",
       "      document.body.appendChild(div_out);\n",
       "      img = document.createElement('img');\n",
       "      div_out.appendChild(img);\n",
       "    }\n",
       "\n",
       "    //  \n",
       "    async function capture(){\n",
       "        return await new Promise(function(resolve, reject){\n",
       "            pendingResolve = resolve;\n",
       "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "            result = canvas.toDataURL('image/jpeg', 0.80);\n",
       "\n",
       "            pendingResolve(result);\n",
       "        })\n",
       "    }\n",
       "\n",
       "    function showimg(imgb64){\n",
       "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
       "    }\n",
       "\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-11c4ed945229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0misDetection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0me3\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTickCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reduce the computation time of Facedetect\n",
    "\n",
    "VideoCapture()\n",
    "eval_js('create()')\n",
    "\n",
    "#application of the Haarcascade for object detection faces\n",
    "CascadeFace = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "margin = 60\n",
    "while True:\n",
    "  bigrect=[]\n",
    "  e1 = cv2.getTickCount()\n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
    "  face = detect(gray, cascade = CascadeFace) # face contains the rectangle\n",
    "  #draw_rects(im, face, (0, 0, 0)) # draw the rectangle\n",
    "  e2 = cv2.getTickCount()\n",
    "  isDetection=False\n",
    "\n",
    "  if len(face)==0:\n",
    "    isFaceDetect=False\n",
    "  else:\n",
    "    isFaceDetect=True\n",
    "\n",
    "  if len(bigrect)==0 and isFaceDetect:\n",
    "    bigrect.append(compute_big_rect(face[0]))\n",
    "    isDetection=True\n",
    "\n",
    "  #this should be the new main cycle instead of the for cycle\n",
    "  while isDetection:\n",
    "    e3 =cv2.getTickCount()\n",
    "    byte = eval_js('capture()')\n",
    "    im = byte2image(byte)\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    roi=gray[bigrect[0][1]:bigrect[0][3], bigrect[0][0]:bigrect[0][2]]\n",
    "    roi_color=im[bigrect[0][1]:bigrect[0][3], bigrect[0][0]:bigrect[0][2]]\n",
    "    newface=detect(roi, cascade = CascadeFace) \n",
    "    \n",
    "    draw_rects(im,bigrect, (0,0,0))\n",
    "    draw_rects(roi_color,newface, (153, 255, 255))\n",
    "    e4 = cv2.getTickCount()\n",
    "\n",
    "    if len(newface)>0:\n",
    "      eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
    "    else:\n",
    "      isDetection=False\n",
    "      bigrect=[]\n",
    "      isFace=False\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iBNavzeDhYF"
   },
   "source": [
    "# **Task 3**\n",
    "\n",
    "For the third task I am going to use facedetect once to get the face and compute the region of interest. I am then switching to camshift to calculate the hystogram of the face. The algoritmh then calculates on the window where its the most probable to get a face and searches in the region of interest where is the face that has the same distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1668520483129,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "LvZcMA2EeThD"
   },
   "outputs": [],
   "source": [
    "def compute_big_rect2(rect): #[x1,y1,x2,y2]\n",
    "\n",
    "  x1=max(0,rect[0]-margin)\n",
    "  y1=max(0,rect[1]-margin)\n",
    "  x2=min(rect[2]+margin,image_max_width)\n",
    "  y2=min(rect[3]+margin,image_max_height)\n",
    "  \n",
    "  return [x1,y1,x2,y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8665,
     "status": "error",
     "timestamp": 1668520492766,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "NxIWGagQoqpL",
    "outputId": "b0aa62db-c673-485f-db60-3b81a8aa8e7d"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    // the function 'create' creates the \"box\" that contains the videostream\n",
       "    // async functions return a promise, they make the code look synchronous, but it's asynchronous and non-blocking behind the scenes\n",
       "    async function create(){ \n",
       "      div = document.createElement('div');\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      video = document.createElement('video');\n",
       "      video.setAttribute('playsinline', '');\n",
       "\n",
       "      div.appendChild(video);\n",
       "      // await is used to call functions, the calling code will stop until the promise is resolved or rejected\n",
       "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
       "      video.srcObject = stream;\n",
       "\n",
       "      // await is used to call functions, here we call the video.play() function\n",
       "      await video.play();\n",
       "\n",
       "      canvas =  document.createElement('canvas');\n",
       "      canvas.width = video.videoWidth;\n",
       "      canvas.height = video.videoHeight;\n",
       "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "\n",
       "      div_out = document.createElement('div');\n",
       "      document.body.appendChild(div_out);\n",
       "      img = document.createElement('img');\n",
       "      div_out.appendChild(img);\n",
       "    }\n",
       "\n",
       "    //  \n",
       "    async function capture(){\n",
       "        return await new Promise(function(resolve, reject){\n",
       "            pendingResolve = resolve;\n",
       "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "            result = canvas.toDataURL('image/jpeg', 0.80);\n",
       "\n",
       "            pendingResolve(result);\n",
       "        })\n",
       "    }\n",
       "\n",
       "    function showimg(imgb64){\n",
       "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
       "    }\n",
       "\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-08042f9a0a49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0misDetection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert the image into GRAY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#task 3 \n",
    "VideoCapture()\n",
    "eval_js('create()')\n",
    "\n",
    "CascadeFace = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "isFaceDetect=False\n",
    "margin = 30\n",
    "\n",
    "while not isFaceDetect:\n",
    "  bigrect=[]\n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
    "  face = detect(gray, cascade = CascadeFace) # face contains the rectangle\n",
    "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) # convert image to HSV\n",
    "  \n",
    "  #control if a face has been detected\n",
    "  if len(face)==0: \n",
    "    isFaceDetect=False\n",
    "  else:\n",
    "    isFaceDetect=True\n",
    "    \n",
    "  if len(bigrect)==0 and isFaceDetect: # if bigrect is empty and a face is found\n",
    "    hsv_roi=hsv[face[0][1]:face[0][3], face[0][0]:face[0][2]]\n",
    "    bigrect.append(compute_big_rect2(face[0])) # compute bigrect and add it to the list\n",
    "  \n",
    "    isDetection=True # set a flag to start camshift\n",
    "    gray_roi = gray[bigrect[0][1]:bigrect[0][3], bigrect[0][0]:bigrect[0][2]] # compute the region of interest\n",
    "    hist=cv2.calcHist([hsv_roi], [0], None, [180], [0,180]) # compute the histogram\n",
    "    plt.hist(gray_roi.ravel(),256,[0,256]) # show the histogram\n",
    "    plt.show()\n",
    "  \n",
    "while isDetection:\n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
    "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
    "\n",
    "  track_window = (bigrect[0][0], bigrect[0][1], bigrect[0][2], bigrect[0][3]) # compute the region of interest (bigrect)\n",
    "  prob = cv2.calcBackProject([hsv], [0], hist, [0, 256], 1) # compute the probability matrix \n",
    "  term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )  # sets some criteria for camshift\n",
    "  track_box, track_window = cv2.CamShift(prob, track_window, term_crit) # perform camshift over the track_window\n",
    "\n",
    "  im[:] = prob[...,np.newaxis] # add probability matrix to the image\n",
    "\n",
    "  draw_rects(im, face, (255, 0, 255))\n",
    "  draw_rects(im, bigrect, (255, 0, 0), 4) # draw the rectangle\n",
    "\n",
    "  try:\n",
    "    cv2.ellipse(im, track_box, (0, 0, 255), 2) # draw the ellipse\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(track_box)\n",
    "\n",
    "  eval_js('showimg(\"{}\")'.format(image2byte(im)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XVP3HcA4Pug"
   },
   "source": [
    "# **Task 4**\n",
    "\n",
    "In the fourth task the goal is to remove the face so we can detect the hands. I am goin to remove the face from the probability map so the algorithm will look at the picture to find where there is a similar distribution of color as the face and it will find the hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1668520494621,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "659FH3DFcgum"
   },
   "outputs": [],
   "source": [
    "#implement\n",
    "def restrict(rect): #[x1,y1,x2,y2]\n",
    "\n",
    "  x1=min(image_max_width,rect[0]+restr)\n",
    "  y1=min(image_max_height,rect[1]+restr)\n",
    "  x2=max(rect[2]-restr,x1)\n",
    "  y2=max(rect[3]-restr,y1)\n",
    "  return [x1,y1,x2,y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 19040,
     "status": "error",
     "timestamp": 1668520515290,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "nloz1W4sfY0m",
    "outputId": "7fe33d10-e7c5-4adc-cf8d-f6011f5fb981"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    // the function 'create' creates the \"box\" that contains the videostream\n",
       "    // async functions return a promise, they make the code look synchronous, but it's asynchronous and non-blocking behind the scenes\n",
       "    async function create(){ \n",
       "      div = document.createElement('div');\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      video = document.createElement('video');\n",
       "      video.setAttribute('playsinline', '');\n",
       "\n",
       "      div.appendChild(video);\n",
       "      // await is used to call functions, the calling code will stop until the promise is resolved or rejected\n",
       "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
       "      video.srcObject = stream;\n",
       "\n",
       "      // await is used to call functions, here we call the video.play() function\n",
       "      await video.play();\n",
       "\n",
       "      canvas =  document.createElement('canvas');\n",
       "      canvas.width = video.videoWidth;\n",
       "      canvas.height = video.videoHeight;\n",
       "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "\n",
       "      div_out = document.createElement('div');\n",
       "      document.body.appendChild(div_out);\n",
       "      img = document.createElement('img');\n",
       "      div_out.appendChild(img);\n",
       "    }\n",
       "\n",
       "    //  \n",
       "    async function capture(){\n",
       "        return await new Promise(function(resolve, reject){\n",
       "            pendingResolve = resolve;\n",
       "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "            result = canvas.toDataURL('image/jpeg', 0.80);\n",
       "\n",
       "            pendingResolve(result);\n",
       "        })\n",
       "    }\n",
       "\n",
       "    function showimg(imgb64){\n",
       "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
       "    }\n",
       "\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ac341ccdd603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0misDetection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;31m#gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#task 4\n",
    "VideoCapture()\n",
    "eval_js('create()')\n",
    "\n",
    "CascadeFace = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "isFaceDetect=False\n",
    "margin = 30\n",
    "restr = 30\n",
    "\n",
    "while not isFaceDetect:\n",
    "  bigrect=[] \n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
    "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
    "  face = detect(gray, cascade = CascadeFace) # face contains the rectangle\n",
    " \n",
    "  if len(face)==0:\n",
    "    isFaceDetect=False\n",
    "  else:\n",
    "    isFaceDetect=True\n",
    "    \n",
    "  if isFaceDetect: # if a face is found\n",
    "    restricted_face = restrict(face[0])\n",
    "    hsv_roi=hsv[restricted_face[1]:restricted_face[3], restricted_face[0]:restricted_face[2]] # !!! better to restrict this face for the histogram\n",
    "    bigrect.append(compute_big_rect2(face[0])) # compute bigrect and add to the list\n",
    "  \n",
    "    isDetection=True # set a flag to start camshift\n",
    "    #gray_roi = gray[bigrect[0][1]:bigrect[0][3], bigrect[0][0]:bigrect[0][2]] # compute the region of interest\n",
    "    hist=cv2.calcHist([hsv_roi], [0], None, [180], [0,180]) # compute the histogram\n",
    "    plt.hist(hsv_roi.ravel(),256,[0,256]) # show the histogram\n",
    "    plt.show()\n",
    "\n",
    "    track_window = (bigrect[0][0], bigrect[0][1], bigrect[0][2], bigrect[0][3]) # compute the region of interest (bigrect)\n",
    "\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 ) # sets some criteria for camshift\n",
    "\n",
    "while isDetection:\n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  #gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
    "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
    "  mask = cv2.inRange(hsv, np.array((0,64,32)), np.array((180,200,200)))\n",
    "\n",
    "  prob = cv2.calcBackProject([hsv], [0], hist, [0, 180], 1) # compute the probability matrix \n",
    "\n",
    "  for i in range(bigrect[0][0], bigrect[0][2]): # iterates between x1, x2\n",
    "    for j in range(bigrect[0][1], bigrect[0][3]): # iterates between y1, y2\n",
    "      prob[j,i]=0 # sets probability matrix to 0 on all points inside bigrect\n",
    "\n",
    "  prob &= mask\n",
    "\n",
    "  track_box, track_window = cv2.CamShift(prob, track_window, term_crit) # perform camshift over the track_window\n",
    "  \n",
    "  im[:] = prob[...,np.newaxis] # add probability matrix to the image\n",
    "\n",
    "  draw_rects(im, [restricted_face], (255, 0, 255))\n",
    " \n",
    "  cv2.ellipse(im, track_box, (255, 0, 0), 2) # draw the ellipse\n",
    "  \n",
    "\n",
    "  eval_js('showimg(\"{}\")'.format(image2byte(im)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mdhdP8UUxXj"
   },
   "source": [
    "# **Task 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D6B0orYxuor"
   },
   "source": [
    "In task 5 the scope is to detect and store the hand as an image in two different sizes: 16x16 and 224x224. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWry8TzJySNQ"
   },
   "source": [
    "We start by asking how many pictures you want to take and how many seconds between each picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "executionInfo": {
     "elapsed": 2159,
     "status": "error",
     "timestamp": 1668520519501,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "Cng9l3Lzj11B",
    "outputId": "d3233abd-8e74-472c-b9f8-9daa818ec2b7"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a128f66808c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPicture_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"How many pictures do you want to take? \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# the number of picture to be taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"How many seconds between each photo? \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;31m# counter of saved images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msaved_images\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "Picture_count=int(input(\"How many pictures do you want to take? \"))# the number of picture to be taken\n",
    "Time=int(input(\"How many seconds between each photo? \"))\n",
    "n_images=0 # counter of saved images\n",
    "saved_images =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZWCzEbQykiM"
   },
   "source": [
    "We continue by actually taking the pictures, cropping them so it takes just the hand and saving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9083,
     "status": "error",
     "timestamp": 1668520561306,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "N5y4dqyuj5oi",
    "outputId": "4cc5fa0f-9ff8-4fe6-bcd5-28395f3348a9"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    // the function 'create' creates the \"box\" that contains the videostream\n",
       "    // async functions return a promise, they make the code look synchronous, but it's asynchronous and non-blocking behind the scenes\n",
       "    async function create(){ \n",
       "      div = document.createElement('div');\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      video = document.createElement('video');\n",
       "      video.setAttribute('playsinline', '');\n",
       "\n",
       "      div.appendChild(video);\n",
       "      // await is used to call functions, the calling code will stop until the promise is resolved or rejected\n",
       "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
       "      video.srcObject = stream;\n",
       "\n",
       "      // await is used to call functions, here we call the video.play() function\n",
       "      await video.play();\n",
       "\n",
       "      canvas =  document.createElement('canvas');\n",
       "      canvas.width = video.videoWidth;\n",
       "      canvas.height = video.videoHeight;\n",
       "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "\n",
       "      div_out = document.createElement('div');\n",
       "      document.body.appendChild(div_out);\n",
       "      img = document.createElement('img');\n",
       "      div_out.appendChild(img);\n",
       "    }\n",
       "\n",
       "    //  \n",
       "    async function capture(){\n",
       "        return await new Promise(function(resolve, reject){\n",
       "            pendingResolve = resolve;\n",
       "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "            result = canvas.toDataURL('image/jpeg', 0.80);\n",
       "\n",
       "            pendingResolve(result);\n",
       "        })\n",
       "    }\n",
       "\n",
       "    function showimg(imgb64){\n",
       "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
       "    }\n",
       "\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAABmCAIAAACx029lAAAEkElEQVR4nO3dMW6zMBQH8AdmZciUqL5Bhq4ZeoGcIDfoWokepD1Ab5ATVOo9yAmQwtShQxeIv+E1T3wkGJOYksL/N1RNINT14z0bg5SArmPI8C8BBVceCly4dHj4W42B34OgjhCCOkII6gghqCOEoI4QgjpCka8DyfUTDA6ZOkLXBhULSYNAtwMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD8BcH/rj+C4wGVUlrrJEmMMXEcx3Es+4dhGARBHMdpmhpj0jRNkkRr3XSo6ApKqdqhlFJKKfnlgg65wGw2s+/AjdFaS5OiyNs3MraE0CWoson7jkObpmkcx9Q9QqYj/tNhBZ9S6/U6TdMoii47s31RSkkDuKnSRYxPdGNMkiTk8as2+zObzb6+vjp95Pn5udP+rTEzZuAvEg3DsCxLzsjD4UBEeZ5vt9vNZiP77HY7IrJUr26a0vGyTNVHnKlxHPedqac5Kmnqp4MutV6vq/1Dx5IrvcSZyv8Ft1Zr7aeqnI0Tl7WzL09PfNkUhiERLRaLj48Pfme1Wn1/f7s3piiKrsFYrVbVlw8PD0T08vKyXC47HacPaZpyM7iLuOs4ZReLxWazeX19Ncbsdrvlcqm13u/3fsov/6XT0J7G9exulurXtZAWRWHZ2hQkHrmlyL+/v8vPwd3f3/Mvxhil1OFw4POeiPb7PRElScIRlY94G//PRrS6yTIsyQ48W+F35vO5JKv8Y604qLVMlX+4KYNvISMtoiiyn6xa6yzLZA7cb/mt7tAaVDrWXjoGdbPZPD4+EtFyuWydrBdFIftIF8iYVHUjKdhqNpt9fn5SJaj26HJQy7L02YjW2ZD9s9IynuwQkdaa5+68yTIzkuPUXv5RfIV69oxsIhMo8lh+hWU2ZG9TdbQIgqAoCh72L7hGtBerUVJKlWXJFfHaL5o/xdPrTh+RsPEVmMiy7OnpyVvLRo2r7s9UtKe/YR9Ea7vJ/IiIJF+5lT/1xC1ZJ5igZ/nPVNYa0eq6V21TLV/LskS0OukrqNfgTJUlsV9bQP9bLNVrmKBasrNah2XrCCa0vrgslg1z86FppaKalDy4yqDb6fgTL9e3Un55Kb82mhJRURSnb7oc7bLL5b/CfiumlxWl1tyqTaOapsp87ZWmKS8Tzufz6tY8z6svJ56dVT7Hqtpib+tu0BM/5bd6+8VvRPlWPnTic8ixhJMLssS+ttZPDcEOguDu7o6OlRbl15HPiZJ9gbDpZqoFX97IDThw1Mvar31rU1Kevnk4HPI8f3t789a4aejlksbXVIjPgO126+Vo0zHY2u9/jQhDWUg6lWWZjxZNSI+LD45xrd5DPd0qa79FUUwwup3uk4veF1zsDyhVb7pRw6MYvNhE05798jqM4869LxN2qsOtzzpNk9a605NHv7H263Jv1fFSJ8syKcLjTs0qfhTU3fAL+hxOxwjxY+k9t+i2dE1TGjyoksS4Y+rRwEHt9LR+zUTOA669PFt07Jwb6hf3cPKwOpGgMqWU+2Oa/wATx1IdAKptzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=156x102 at 0x7FBCBCE61250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c832b2fd1351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0msave_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Do you want to save the image Y or N: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m#cv2_imshow(cropped_image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "VideoCapture()\n",
    "eval_js('create()')\n",
    "\n",
    "CascadeFace = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "isFaceDetect=False\n",
    "margin = 30\n",
    "restr = 30\n",
    "\n",
    "while not isFaceDetect:\n",
    "  bigrect=[] \n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
    "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
    "  face = detect(gray, cascade = CascadeFace) # face contains the rectangle\n",
    " \n",
    "  if len(face)==0:\n",
    "    isFaceDetect=False\n",
    "  else:\n",
    "    isFaceDetect=True\n",
    "    \n",
    "  if isFaceDetect: # if a face is found\n",
    "    restricted_face = restrict(face[0])\n",
    "    hsv_roi=hsv[restricted_face[1]:restricted_face[3], restricted_face[0]:restricted_face[2]] # !!! better to restrict this face for the histogram\n",
    "    mask_roi = cv2.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
    "    bigrect.append(compute_big_rect2(face[0])) # compute bigrect and add to the list\n",
    "  \n",
    "    isDetection=True # set a flag to start camshift\n",
    "    hist=cv2.calcHist([hsv_roi], [0], mask_roi, [180], [0,180]) # compute the histogram\n",
    "    hist=cv2.normalize(hist, hist,  alpha=0, beta=255, norm_type=cv2.NORM_MINMAX) # normalize the histogram \n",
    "    plt.hist(hsv_roi.ravel(),256,[0,256]) # show the histogram\n",
    "    plt.show()\n",
    "\n",
    "    track_window = (bigrect[0][0], bigrect[0][1], bigrect[0][2], bigrect[0][3]) # compute the region of interest (bigrect)\n",
    "\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 ) # sets some criteria for camshift\n",
    "\n",
    "while isDetection:\n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
    "  mask = cv2.inRange(hsv, np.array((0,64,32)), np.array((180,200,200)))\n",
    "  raw_im = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n",
    "  prob = cv2.calcBackProject([hsv], [0], hist, [0, 180], 1) # compute the probability matrix \n",
    "  \n",
    "\n",
    "  for i in range(bigrect[0][0], bigrect[0][2]): # iterates between x1, x2\n",
    "    for j in range(bigrect[0][1], bigrect[0][3]): # iterates between y1, y2\n",
    "      prob[j,i]=0 # sets probability matrix to 0 on all points inside bigrect\n",
    "\n",
    "  prob &= mask\n",
    "\n",
    "  track_box, track_window = cv2.CamShift(prob, track_window, term_crit) # perform camshift over the track_window\n",
    "  \n",
    "  im[:] = prob[...,np.newaxis] # add probability matrix to the image\n",
    "\n",
    "  draw_rects(im, [restricted_face], (255, 0, 255))\n",
    " \n",
    "  #cv2.ellipse(im, track_box, (255, 0, 0), 2) # draw the ellipse\n",
    "\n",
    "  eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
    "  if n_images==Picture_count:\n",
    "    break\n",
    "  else:\n",
    "    time.sleep(Time)\n",
    "  #we crop the image to have only the hand\n",
    "  cropped_image = im[int(track_window[1]):int(track_window[1]+track_window[3]), int(track_window[0]):int(track_window[0]+track_window[2])]\n",
    "\n",
    "  cv2_imshow(cropped_image)\n",
    "  \n",
    "  save_image=input(\"Do you want to save the image Y or N: \")\n",
    "  #cv2_imshow(cropped_image)\n",
    "  \n",
    "  #eval_js('showimg(\"{}\")'.format(image2byte(hand)))\n",
    "  \n",
    "  if str.lower(save_image) == 'y':\n",
    "\n",
    "    saved_images.append(cropped_image)\n",
    "    print(\"Picture\",n_images+1,\"was stored\")\n",
    "    #print(cropped_image)\n",
    "    n_images+=1\n",
    "    continue\n",
    "  else:\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "error",
     "timestamp": 1668520564289,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "DszbykbHpOp8",
    "outputId": "f168573d-1fb2-4915-9b40-ef99be66dfb7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fd59066eaa10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#to see all the saved pictures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msaved_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saved_images' is not defined"
     ]
    }
   ],
   "source": [
    "#to see all the saved pictures\n",
    "for image in saved_images:\n",
    "  cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55xIe3bnzokO"
   },
   "source": [
    "In the variable img_path I saved the path to the folder containing the pictures, since we made three different datasets I manually changed the folder name when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1668520585137,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "NQHe64jaoBZO"
   },
   "outputs": [],
   "source": [
    "img_path = \"/content/gdrive/MyDrive/Computer Vision Ilaria/images/data1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfsDbojtzuQt"
   },
   "source": [
    "resize_images is used to resize all the images saved into the two sizes, name the pictures with letter+id+size and save them to the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1668520587237,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "DRXuoFG8pmHg"
   },
   "outputs": [],
   "source": [
    "def resize_images(images, letter): \n",
    "  sizes = [16, 224]\n",
    "  for id,image in enumerate(images):\n",
    "    for size in sizes:\n",
    "      file_name = letter.upper()+\"_\" + str(id) +\"_\"+  str(size) #give the pic a name\n",
    "      print(file_name) #print the name\n",
    "      try:\n",
    "        resized_img = cv2.resize(image, (size, size), interpolation = cv2.INTER_AREA) #resize image\n",
    "      except:\n",
    "        break\n",
    "      cv2_imshow(resized_img) #print resized pic\n",
    "      cv2.imwrite(img_path + file_name + \".jpg\", resized_img) #save image to folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "error",
     "timestamp": 1668520588415,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "7NIdRsaDqyeD",
    "outputId": "b18a4782-70e7-4232-bc4d-02a7d98cada8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-607685c86cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#calling the function, change the letter when needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'saved_images' is not defined"
     ]
    }
   ],
   "source": [
    "#calling the function, change the letter when needed\n",
    "resize_images(saved_images, 'N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyhd_aI3VRiq"
   },
   "source": [
    "# **Task 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bcf7mfii0-eZ"
   },
   "source": [
    "In task 6 we create our dataset, the letters chosen were M, N and W. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1668520590573,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "qAsLbhGkeVzY"
   },
   "outputs": [],
   "source": [
    "# changes the shape of the array of the 16x16 images and appends it into the text file dataset.txt\n",
    "def Resize_and_Write(id, letter, dataset):\n",
    "\n",
    "\n",
    "  #given an id and letter, reads the image and converts it into a list\n",
    "  img = cv2.imread(img_path +letter+\"_\" + str(id) + \"_16.jpg\") #\n",
    "  res = cv2.resize(img, dsize=(1,256), interpolation=cv2.INTER_CUBIC) # 16x16 to (1,256) 1row | 256 columns\n",
    "  flat_res = [str(a[0][0]) for a in res] # 1 with 256 elem\n",
    "  \n",
    "\n",
    "  is_empty = False\n",
    "  try:\n",
    "    with  open(\"/content/gdrive/MyDrive/Computer Vision Ilaria/\" + dataset, \"r\") as fileobj: # open the dataset in read mode\n",
    "      if len(fileobj.readlines())==0: # checks if the dataset is empty\n",
    "        is_empty = True\n",
    "  except:\n",
    "    is_empty = True # if the dataset does not exist, set the empty variable to true\n",
    "  with open(\"/content/gdrive/MyDrive/Computer Vision Ilaria/\" + dataset, \"a+\") as fileobj: # open the dataset in append mode\n",
    "    L_list=[letter, *flat_res] # create a list with the letter as first element, followed by all the values of the pixels\n",
    "    str_list = str(L_list)\n",
    "    str_list = str_list.replace('\\'', '')[1:-1] # remove the quotes and the brackets\n",
    "    if is_empty:\n",
    "      fileobj.write(str_list)#Creates file if file doesnt exist/also appends \n",
    "    else:\n",
    "      fileobj.write(\"\\n\" + str_list)#Creates file if file doesnt exist/also appends\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1668520592595,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "q_0JnHIFlT1j"
   },
   "outputs": [],
   "source": [
    "letters = [\"M\", \"N\", \"W\"]\n",
    "dataset_1 = \"dataset1.txt\"\n",
    "dataset_2 = \"dataset2.txt\"\n",
    "dataset_3 = \"dataset3.txt\"\n",
    "\n",
    "#change when needed\n",
    "n_images = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQvNY-wc2aCA"
   },
   "source": [
    "Below we just change the name of the dataset as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 11195,
     "status": "error",
     "timestamp": 1668520605738,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "I9Y8ZZAFmOL6",
    "outputId": "346bab43-0d2d-413b-b744-82e8e1ba0f42"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7eedcd7896d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mResize_and_Write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"N\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-42bb1897ed96>\u001b[0m in \u001b[0;36mResize_and_Write\u001b[0;34m(id, letter, dataset)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#given an id and letter, reads the image and converts it into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_16.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 16x16 to (1,256) 1row | 256 columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mflat_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 1 with 256 elem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n_images):\n",
    "  Resize_and_Write(i, \"N\", dataset_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cAd9ZrJ2k5Q"
   },
   "source": [
    "At the end we shuffle the lines of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1668520605738,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "rQaTvAvRgObr"
   },
   "outputs": [],
   "source": [
    "lines = open('/content/gdrive/MyDrive/Computer Vision Ilaria/dataset2.txt').readlines()\n",
    "random.shuffle(lines)\n",
    "open('/content/gdrive/MyDrive/Computer Vision Ilaria/dataset2.txt', 'w').writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIBJo1pqnTuf"
   },
   "source": [
    "# **Task 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQLneFevj6Lx"
   },
   "source": [
    "In the following task we build our MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1668520609698,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "SeIypusennlF"
   },
   "outputs": [],
   "source": [
    "dataset_file_path = \"/content/gdrive/MyDrive/Computer Vision Ilaria/dataset1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1668520609698,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "GzsswWEnnSK1"
   },
   "outputs": [],
   "source": [
    "#The following function takes the dataset file and returns two arrays: samples and letters.\n",
    "def load_dataset(dataset_file_path):\n",
    "    a = np.loadtxt(dataset_file_path, delimiter=',', converters={ 0 : lambda ch : ord(ch)-ord('A') })\n",
    "    samples, letters = a[:,1:], a[:,0] # samples takes all the rows and all the columns except the first column\n",
    "    # letters: keep only the first column\n",
    "    return samples, letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1Ky49hjkEkF"
   },
   "source": [
    "I decided to split the dataset 70% for the training and 30% for the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1668520612488,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "EOFlz6Kc4IL6",
    "outputId": "1e28e9ae-6e02-4ce0-ada8-c3f6244dcd58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 256)\n"
     ]
    }
   ],
   "source": [
    "#here we split the dataset for training and validation\n",
    "train_ratio = 0.7\n",
    "samples, letters = load_dataset(dataset_file_path)\n",
    "n_train_samples = int(len(samples) * train_ratio)\n",
    "x_train, y_train = samples[:n_train_samples], letters[:n_train_samples] # keeps only the first 70% of rows\n",
    "x_val, y_val = samples[n_train_samples:], letters[n_train_samples:] # keeps only the last 30%\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8877,
     "status": "ok",
     "timestamp": 1653252989111,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -120
    },
    "id": "TThfhk8dvaZK",
    "outputId": "c51be301-dcc1-4c74-ad28-85918b16ca92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 train samples\n",
      "90 test samples\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_65 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 26)                2626      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,626\n",
      "Trainable params: 58,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 1s 35ms/step - loss: 2.2507 - accuracy: 0.2429 - val_loss: 1.2102 - val_accuracy: 0.3667\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.5088 - accuracy: 0.3476 - val_loss: 1.0607 - val_accuracy: 0.5778\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2057 - accuracy: 0.4952 - val_loss: 0.9629 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1762 - accuracy: 0.5000 - val_loss: 0.9156 - val_accuracy: 0.6111\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1248 - accuracy: 0.5000 - val_loss: 0.9219 - val_accuracy: 0.6556\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0773 - accuracy: 0.5857 - val_loss: 0.9526 - val_accuracy: 0.6444\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0444 - accuracy: 0.5714 - val_loss: 0.9427 - val_accuracy: 0.6333\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.0306 - accuracy: 0.6238 - val_loss: 0.8078 - val_accuracy: 0.6222\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9898 - accuracy: 0.6095 - val_loss: 0.8126 - val_accuracy: 0.6222\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8735 - accuracy: 0.6905 - val_loss: 0.7677 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9067 - accuracy: 0.6571 - val_loss: 0.7639 - val_accuracy: 0.6222\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9593 - accuracy: 0.6190 - val_loss: 0.7602 - val_accuracy: 0.7111\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7943 - accuracy: 0.6810 - val_loss: 0.8430 - val_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8768 - accuracy: 0.6238 - val_loss: 0.8141 - val_accuracy: 0.6000\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8102 - accuracy: 0.6571 - val_loss: 0.7187 - val_accuracy: 0.7111\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.7950 - accuracy: 0.6619 - val_loss: 0.7312 - val_accuracy: 0.6556\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8393 - accuracy: 0.6810 - val_loss: 0.7763 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.7598 - accuracy: 0.7095 - val_loss: 0.7385 - val_accuracy: 0.6778\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7608 - accuracy: 0.6952 - val_loss: 0.7886 - val_accuracy: 0.6333\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7022 - accuracy: 0.7381 - val_loss: 0.7656 - val_accuracy: 0.6556\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7004 - accuracy: 0.7238 - val_loss: 0.7747 - val_accuracy: 0.7111\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7435 - accuracy: 0.7143 - val_loss: 0.7378 - val_accuracy: 0.6889\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6890 - accuracy: 0.7333 - val_loss: 0.7263 - val_accuracy: 0.6444\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7291 - accuracy: 0.7048 - val_loss: 0.7207 - val_accuracy: 0.6667\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6300 - accuracy: 0.7619 - val_loss: 0.7048 - val_accuracy: 0.7111\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7065 - accuracy: 0.6952 - val_loss: 0.7127 - val_accuracy: 0.6333\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6399 - accuracy: 0.7619 - val_loss: 0.7189 - val_accuracy: 0.7000\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6585 - accuracy: 0.7286 - val_loss: 0.7868 - val_accuracy: 0.6444\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6357 - accuracy: 0.7476 - val_loss: 0.7183 - val_accuracy: 0.7333\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6160 - accuracy: 0.7714 - val_loss: 0.7531 - val_accuracy: 0.6222\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.5840 - accuracy: 0.7714 - val_loss: 0.7478 - val_accuracy: 0.7000\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6215 - accuracy: 0.7429 - val_loss: 0.7675 - val_accuracy: 0.6222\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6184 - accuracy: 0.7524 - val_loss: 0.8251 - val_accuracy: 0.7333\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.5871 - accuracy: 0.7619 - val_loss: 0.7409 - val_accuracy: 0.6333\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.5420 - accuracy: 0.8000 - val_loss: 0.7480 - val_accuracy: 0.7000\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.5206 - accuracy: 0.7810 - val_loss: 0.7988 - val_accuracy: 0.6444\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.5495 - accuracy: 0.7714 - val_loss: 0.7753 - val_accuracy: 0.6333\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5057 - accuracy: 0.8190 - val_loss: 0.8756 - val_accuracy: 0.6333\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.5440 - accuracy: 0.8000 - val_loss: 0.7603 - val_accuracy: 0.7000\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.5053 - accuracy: 0.8190 - val_loss: 0.8256 - val_accuracy: 0.6333\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.5163 - accuracy: 0.8048 - val_loss: 0.8276 - val_accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.5859 - accuracy: 0.7619 - val_loss: 0.8121 - val_accuracy: 0.7444\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.4017 - accuracy: 0.8524 - val_loss: 0.8240 - val_accuracy: 0.6333\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4137 - accuracy: 0.8429 - val_loss: 0.8793 - val_accuracy: 0.6778\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.4698 - accuracy: 0.8286 - val_loss: 0.9125 - val_accuracy: 0.6778\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.4296 - accuracy: 0.8429 - val_loss: 0.8427 - val_accuracy: 0.6889\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.3856 - accuracy: 0.8238 - val_loss: 0.8031 - val_accuracy: 0.7111\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.4884 - accuracy: 0.7810 - val_loss: 0.9023 - val_accuracy: 0.6556\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.4102 - accuracy: 0.8190 - val_loss: 0.8245 - val_accuracy: 0.7222\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.4865 - accuracy: 0.7952 - val_loss: 0.8438 - val_accuracy: 0.6778\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3540 - accuracy: 0.8571 - val_loss: 0.8660 - val_accuracy: 0.6667\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3890 - accuracy: 0.8524 - val_loss: 0.8820 - val_accuracy: 0.6667\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3680 - accuracy: 0.8619 - val_loss: 0.8827 - val_accuracy: 0.6889\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4244 - accuracy: 0.8476 - val_loss: 0.9319 - val_accuracy: 0.6667\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3345 - accuracy: 0.8571 - val_loss: 0.8708 - val_accuracy: 0.6778\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3285 - accuracy: 0.8667 - val_loss: 0.9333 - val_accuracy: 0.7111\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3062 - accuracy: 0.8905 - val_loss: 0.9997 - val_accuracy: 0.7111\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3744 - accuracy: 0.8381 - val_loss: 1.0384 - val_accuracy: 0.6222\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.3563 - accuracy: 0.8524 - val_loss: 0.9807 - val_accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.3651 - accuracy: 0.8571 - val_loss: 1.0895 - val_accuracy: 0.6000\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.3119 - accuracy: 0.8714 - val_loss: 1.0538 - val_accuracy: 0.6889\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2554 - accuracy: 0.9238 - val_loss: 1.1879 - val_accuracy: 0.6889\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3877 - accuracy: 0.8714 - val_loss: 1.0552 - val_accuracy: 0.6889\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2705 - accuracy: 0.8714 - val_loss: 1.1553 - val_accuracy: 0.6889\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2405 - accuracy: 0.9048 - val_loss: 1.2347 - val_accuracy: 0.6556\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2691 - accuracy: 0.8905 - val_loss: 1.1620 - val_accuracy: 0.6556\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2513 - accuracy: 0.9048 - val_loss: 1.0272 - val_accuracy: 0.6778\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2330 - accuracy: 0.9095 - val_loss: 1.3262 - val_accuracy: 0.6778\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3705 - accuracy: 0.8571 - val_loss: 1.1163 - val_accuracy: 0.6889\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2374 - accuracy: 0.9190 - val_loss: 1.2472 - val_accuracy: 0.6333\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2045 - accuracy: 0.9238 - val_loss: 1.1961 - val_accuracy: 0.6889\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2643 - accuracy: 0.9000 - val_loss: 1.3506 - val_accuracy: 0.6778\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1883 - accuracy: 0.9190 - val_loss: 1.3089 - val_accuracy: 0.7222\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2066 - accuracy: 0.9048 - val_loss: 1.3868 - val_accuracy: 0.6889\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1902 - accuracy: 0.9190 - val_loss: 1.4723 - val_accuracy: 0.7222\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2055 - accuracy: 0.9333 - val_loss: 1.4521 - val_accuracy: 0.6444\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2050 - accuracy: 0.9381 - val_loss: 1.2196 - val_accuracy: 0.6778\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2011 - accuracy: 0.9333 - val_loss: 1.2569 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.3871 - accuracy: 0.8571 - val_loss: 1.3685 - val_accuracy: 0.6222\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1467 - accuracy: 0.9476 - val_loss: 1.2033 - val_accuracy: 0.6778\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1305 - accuracy: 0.9476 - val_loss: 1.8114 - val_accuracy: 0.6667\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.3594 - accuracy: 0.8810 - val_loss: 1.8520 - val_accuracy: 0.6000\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1644 - accuracy: 0.9286 - val_loss: 1.4896 - val_accuracy: 0.6556\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1932 - accuracy: 0.9381 - val_loss: 1.4712 - val_accuracy: 0.6556\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1110 - accuracy: 0.9667 - val_loss: 1.4623 - val_accuracy: 0.6889\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1630 - accuracy: 0.9143 - val_loss: 1.5673 - val_accuracy: 0.6222\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1449 - accuracy: 0.9286 - val_loss: 1.4039 - val_accuracy: 0.7111\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1920 - accuracy: 0.9143 - val_loss: 1.3956 - val_accuracy: 0.6889\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1269 - accuracy: 0.9476 - val_loss: 1.6494 - val_accuracy: 0.7000\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2722 - accuracy: 0.8762 - val_loss: 1.4223 - val_accuracy: 0.6556\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0634 - accuracy: 0.9857 - val_loss: 1.5443 - val_accuracy: 0.6556\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0912 - accuracy: 0.9714 - val_loss: 1.7470 - val_accuracy: 0.6778\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1438 - accuracy: 0.9381 - val_loss: 1.7298 - val_accuracy: 0.6667\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1036 - accuracy: 0.9762 - val_loss: 1.9352 - val_accuracy: 0.6778\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1290 - accuracy: 0.9381 - val_loss: 1.7149 - val_accuracy: 0.6778\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0727 - accuracy: 0.9762 - val_loss: 1.8455 - val_accuracy: 0.6667\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0805 - accuracy: 0.9810 - val_loss: 2.0408 - val_accuracy: 0.6333\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3649 - accuracy: 0.8857 - val_loss: 1.5870 - val_accuracy: 0.6889\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1265 - accuracy: 0.9429 - val_loss: 1.5724 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.0825 - accuracy: 0.9571 - val_loss: 1.9620 - val_accuracy: 0.6556\n",
      "Validation loss: 1.9620263576507568\n",
      "Validation accuracy: 0.6555555462837219\n"
     ]
    }
   ],
   "source": [
    "num_classes = 26 # number of letters\n",
    "epochs = 100 # number of training s\n",
    "\n",
    "\n",
    "#here we split the dataset for training and validation\n",
    "train_ratio = 0.7\n",
    "samples, letters = load_dataset(dataset_file_path)\n",
    "n_train_samples = int(len(samples) * train_ratio)\n",
    "x_train, y_train = samples[:n_train_samples], letters[:n_train_samples] # keeps only the first 70% of rows\n",
    "x_val, y_val = samples[n_train_samples:], letters[n_train_samples:] # keeps only the last 30%\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_train /= 255 # x_train=x_train/255    normalize the numbers in the interval [0,1]\n",
    "x_val /= 255\n",
    "print(x_train.shape[0], 'train samples') # print the number of images\n",
    "print(x_val.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes) # converts the prediction letter into a number encoding for that letter\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(100, activation='relu', input_shape=(256,))) # add a relu layer\n",
    "model.add(Dense(100, activation='relu')) # add anothe relu\n",
    "model.add(Dropout(0.4)) # add a regularizator\n",
    "model.add(Dense(100, activation='relu')) # add anothe relu\n",
    "model.add(Dense(100, activation='relu')) # add anothe relu \n",
    "model.add(Dropout(0.4)) # add a regularizator\n",
    "model.add(Dense(num_classes, activation='softmax'))  # add a final softmax layer\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,   \n",
    "                    epochs=epochs,\n",
    "                    verbose=1, # if verbose=1 print data for each epoch\n",
    "                    validation_data=(x_val, y_val)) # train the network\n",
    "score = model.evaluate(x_val, y_val, verbose=0) # evaluate the obtained model  on the validation set.  #model and dataset\n",
    "print('Validation loss:', score[0]) # print the loss\n",
    "print('Validation accuracy:', score[1]) # print the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9FWTOSzlNLe"
   },
   "source": [
    "After I trained the model, I save the model and weights in two files in the shared folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "error",
     "timestamp": 1668520618249,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "hqx2utB0xukl",
    "outputId": "83e76e94-7f38-4cb9-8935-70e8a83e96b8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2127f2f3de66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/Computer Vision Ilaria/model1.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/Computer Vision Ilaria/model1_weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model to disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"/content/gdrive/MyDrive/Computer Vision Ilaria/model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"/content/gdrive/MyDrive/Computer Vision Ilaria/model1_weights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLW0Hr3y5drv"
   },
   "source": [
    "### **Comparison**\n",
    "\n",
    "After training the 3 models we are going to see how each models performs on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1668520621414,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "gssB7uI_36h0"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(path, model):\n",
    "  train_ratio = 0.7\n",
    "  samples, letters = load_dataset(path) # load the dataset\n",
    "  n_train_samples = int(len(samples) * train_ratio)\n",
    "  x_train, y_train = samples[:n_train_samples], letters[:n_train_samples] # keeps only the first 70% of rows\n",
    "  x_val, y_val = samples[n_train_samples:], letters[n_train_samples:] # keeps only the last 30%\n",
    "  x_train = x_train.astype('float32')\n",
    "  x_val = x_val.astype('float32')\n",
    "  x_train /= 255 # x_train=x_train/255    normalize the numbers in the interval [0,1]\n",
    "  x_val /= 255\n",
    "  \n",
    "  print(x_train.shape[0], 'train samples') # print the number of images\n",
    "  print(x_val.shape[0], 'test samples')\n",
    "\n",
    "  # convert class vectors to binary class matrices\n",
    "  y_train = tf.keras.utils.to_categorical(y_train, num_classes) # converts the prediction letter into a number encoding for that letter\n",
    "  y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "  score = model.evaluate(x_val, y_val, verbose=0) # evaluate the model on the validation set.  #model and dataset\n",
    "  print('Validation loss:', score[0]) # print the loss\n",
    "  print('Validation accuracy:', score[1]) # print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZQ5Uvca4Qeg"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "g_path = \"/content/gdrive/MyDrive/Computer Vision Ilaria/\"\n",
    "# load json and create model\n",
    "json_file = open(g_path+'model3.json', 'r') \n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.summary()\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(g_path+\"model3_weights.h5\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dataset_file_paths = [g_path+\"dataset1.txt\", g_path+\"dataset2.txt\", g_path+\"dataset3.txt\"]\n",
    "for path in dataset_file_paths:\n",
    "  print(path)\n",
    "  evaluate_model(path, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtPa8f0WpW9m"
   },
   "source": [
    "Reminder:\n",
    "\n",
    "Dataset 1: 3 letters with equal number of pictures and a lot of variability \\\n",
    "Dataset 2: 3 letters with unbalanced number of pictures (50 - 100 -150) and a lot of variability \\\n",
    "Dataset 3: 3 letters with equal number of pictures and one of them with no variability (N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1DfgSb8mCWW"
   },
   "source": [
    "**Model 1**\n",
    "\n",
    "**dataset1.txt** \\\n",
    "210 train samples - 90 test samples \\\n",
    "Validation loss: 1.4552514553070068 \\\n",
    "Validation accuracy: 0.6555555462837219\n",
    "\n",
    "**dataset2.txt** \\\n",
    "244 train samples - 106 test samples \\\n",
    "Validation loss: 0.9062689542770386 \\\n",
    "Validation accuracy: 0.8301886916160583\n",
    "\n",
    "**dataset3.txt** \\\n",
    "210 train samples - 90 test samples \\\n",
    "Validation loss: 0.669061005115509 \\\n",
    "Validation accuracy: 0.8444444537162781\n",
    "\n",
    "*Model 1 performs best with dataset 2 and 3, this is probably due to the unbalanced number of pictures in dataset 2 and lack of variability in dataset 3.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD2HGGkzmST9"
   },
   "source": [
    "**Model 2**\n",
    "\n",
    "**dataset1.txt** \\\n",
    "210 train samples - 90 test samples \\\n",
    "Validation loss: 1.7094557285308838 \\\n",
    "Validation accuracy: 0.7666666507720947 \n",
    "\n",
    "**dataset2.txt** \\\n",
    "244 train samples - 106 test samples \\\n",
    "Validation loss: 0.9224103689193726 \\\n",
    "Validation accuracy: 0.8396226167678833 \n",
    "\n",
    "**dataset3.txt** \\\n",
    "210 train samples - 90 test samples \\\n",
    "Validation loss: 1.2521220445632935 \\\n",
    "Validation accuracy: 0.7555555701255798\n",
    "\n",
    "*Model 2 performs best with its own dataset. To make it simple, it's probably easier for the model to guess the right letter when one letter has such an high number of pictures compared to the others. There's a lot more probability that it's going to be the letter with the highest number of pictures.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hMqSlyLmUu6"
   },
   "source": [
    "**Model 3**\n",
    "\n",
    "**dataset1.txt** \\\n",
    "210 train samples - 90 test samples \\\n",
    "Validation loss: 1.2043957710266113 \\\n",
    "Validation accuracy: 0.7888888716697693\n",
    "\n",
    "**dataset2.txt** \\\n",
    "244 train samples - 106 test samples \\\n",
    "Validation loss: 1.269263744354248 \\\n",
    "Validation accuracy: 0.7641509175300598\n",
    "\n",
    "**dataset3.txt** \\\n",
    "210 train samples - 90 test samples \\\n",
    "Validation loss: 1.8944649696350098 \\\n",
    "Validation accuracy: 0.699999988079071\n",
    "\n",
    "*Model 3 performs best with datase 1 and 2.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwW_luDwzZh2"
   },
   "source": [
    "# **Task 8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTs9X_MDu_Fl"
   },
   "source": [
    "Test phase: for the test phase I am going to use model 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1668520624873,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "xXI5rFB1zchf"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('/content/gdrive/MyDrive/Computer Vision Ilaria/model1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json) #not used\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/content/gdrive/MyDrive/Computer Vision Ilaria/model1_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1668520627595,
     "user": {
      "displayName": "ilaria enache",
      "userId": "11897907361892988235"
     },
     "user_tz": -60
    },
    "id": "R4_jFwFmzsSG",
    "outputId": "35fb01bf-0566-485f-e333-931fd6502c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 242ms/step\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "prediction = loaded_model.predict(x_train[0:1,:]) # where hand_image is the probability image of your hand of size (1,256)\n",
    "prediction = prediction.argmax()\n",
    "predicted_letter = chr(ord('A') + prediction)\n",
    "print(predicted_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Em5-DeC2vmZu"
   },
   "source": [
    "I show the hand to the camera doing some of the letters with which I trained the model.\\\n",
    "The program finds my hand, generates a gray scale image of the probability of your hand, resize the image to (1,256).\\\n",
    "Pass this image to the loaded model and predict.\\\n",
    "I show the prediction with a text in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TsEmaFHZz0ft",
    "outputId": "1a4bd365-1199-43ac-cdea-84d947cbd739"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    // the function 'create' creates the \"box\" that contains the videostream\n",
       "    // async functions return a promise, they make the code look synchronous, but it's asynchronous and non-blocking behind the scenes\n",
       "    async function create(){ \n",
       "      div = document.createElement('div');\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      video = document.createElement('video');\n",
       "      video.setAttribute('playsinline', '');\n",
       "\n",
       "      div.appendChild(video);\n",
       "      // await is used to call functions, the calling code will stop until the promise is resolved or rejected\n",
       "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
       "      video.srcObject = stream;\n",
       "\n",
       "      // await is used to call functions, here we call the video.play() function\n",
       "      await video.play();\n",
       "\n",
       "      canvas =  document.createElement('canvas');\n",
       "      canvas.width = video.videoWidth;\n",
       "      canvas.height = video.videoHeight;\n",
       "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "\n",
       "      div_out = document.createElement('div');\n",
       "      document.body.appendChild(div_out);\n",
       "      img = document.createElement('img');\n",
       "      div_out.appendChild(img);\n",
       "    }\n",
       "\n",
       "    //  \n",
       "    async function capture(){\n",
       "        return await new Promise(function(resolve, reject){\n",
       "            pendingResolve = resolve;\n",
       "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
       "            result = canvas.toDataURL('image/jpeg', 0.80);\n",
       "\n",
       "            pendingResolve(result);\n",
       "        })\n",
       "    }\n",
       "\n",
       "    function showimg(imgb64){\n",
       "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
       "    }\n",
       "\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATxUlEQVR4nO3df4xd5X3n8fen5kerJlpMmUVe21rTrLcVWakGzQJVoyqbKGDcP0ykbgR/BDdi5a4EUiJ1V2vaP6DJok1Xm6CNlCI5whtTZUNRkwgr0KUuRYryBz8G1nEwlGUCRNhy8LQmJFG0dKHf/eM+Vm+dufPLd2Y887xf0tWc+z3Pufd5fGY+99znnHudqkKS1IefW+0OSJJWjqEvSR0x9CWpI4a+JHXE0Jekjlyw2h2Yy2WXXVbbtm1b7W5I0pry7LPP/k1VTcy27rwO/W3btjE1NbXa3ZCkNSXJ90etc3pHkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuifZdu+R1a7C5K0bAx9SeqIoS9JHZk39JP8fJKnk3wnybEkf9jqX07yapIj7baj1ZPkC0mmkxxNcvXQY+1J8nK77Vm+YUmSZrOQb9l8G/hQVf0kyYXAt5P8eVv3H6vqz85qfyOwvd2uBe4Drk1yKXAXMAkU8GySQ1X15jgGIkma37xH+jXwk3b3wnarOTbZDTzQtnsSuCTJJuAG4HBVnW5BfxjYeW7dlyQtxoLm9JNsSHIEOMUguJ9qq+5pUzj3Jrm41TYDrw9tfrzVRtXPfq69SaaSTM3MzCxyOJKkuSwo9Kvq3araAWwBrknyr4A7gV8F/jVwKfCfxtGhqtpfVZNVNTkxMet//CJJWqJFXb1TVT8EngB2VtXJNoXzNvA/gGtasxPA1qHNtrTaqLokaYUs5OqdiSSXtOVfAD4C/HWbpydJgJuA59smh4Bb21U81wFvVdVJ4DHg+iQbk2wErm81SdIKWcjVO5uAg0k2MHiReKiqvpnkr5JMAAGOAP++tX8U2AVMAz8FPgFQVaeTfAZ4prX7dFWdHt9QJEnzmTf0q+oocNUs9Q+NaF/A7SPWHQAOLLKPkqQx8RO5Q/zeHUnrnaEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakj84Z+kp9P8nSS7yQ5luQPW/2KJE8lmU7yp0kuavWL2/3ptn7b0GPd2eovJblhuQYlSZrdQo703wY+VFW/BuwAdia5Dvgj4N6q+hfAm8Btrf1twJutfm9rR5IrgZuB9wM7gT9OsmGcg5EkzW3e0K+Bn7S7F7ZbAR8C/qzVDwI3teXd7T5t/YeTpNUfrKq3q+pVYBq4ZiyjkCQtyILm9JNsSHIEOAUcBr4H/LCq3mlNjgOb2/Jm4HWAtv4t4JeG67NsI0laAQsK/ap6t6p2AFsYHJ3/6nJ1KMneJFNJpmZmZpbraSSpS4u6eqeqfgg8Afw6cEmSC9qqLcCJtnwC2ArQ1v8T4G+H67NsM/wc+6tqsqomJyYmFtM9SdI8FnL1zkSSS9ryLwAfAV5kEP6/3ZrtAR5uy4fafdr6v6qqavWb29U9VwDbgafHNRBJ0vwumL8Jm4CD7UqbnwMeqqpvJnkBeDDJfwb+N3B/a38/8CdJpoHTDK7YoaqOJXkIeAF4B7i9qt4d73AkSXOZN/Sr6ihw1Sz1V5jl6puq+r/Avx3xWPcA9yy+m5KkcfATuZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6si8oZ9ka5InkryQ5FiST7b63UlOJDnSbruGtrkzyXSSl5LcMFTf2WrTSfYtz5AkSaNcsIA27wC/V1XPJXkv8GySw23dvVX134YbJ7kSuBl4P/DPgL9M8i/b6i8CHwGOA88kOVRVL4xjIJKk+c17pF9VJ6vqubb8Y+BFYPMcm+wGHqyqt6vqVWAauKbdpqvqlar6O+DB1va8s23fI6vdBUlaFoua00+yDbgKeKqV7khyNMmBJBtbbTPw+tBmx1ttVP3s59ibZCrJ1MzMzGK6J0max4JDP8l7gK8Bn6qqHwH3Ae8DdgAngc+No0NVtb+qJqtqcmJiYhwPKUlqFjKnT5ILGQT+V6rq6wBV9cbQ+i8B32x3TwBbhzbf0mrMUZckrYCFXL0T4H7gxar6/FB901CzjwLPt+VDwM1JLk5yBbAdeBp4Btie5IokFzE42XtoPMOQJC3EQo70fwP4OPDdJEda7feBW5LsAAp4DfhdgKo6luQh4AUGV/7cXlXvAiS5A3gM2AAcqKpjYxyLJGke84Z+VX0byCyrHp1jm3uAe2apPzrXdpKk5eUnciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1JF5Qz/J1iRPJHkhybEkn2z1S5McTvJy+7mx1ZPkC0mmkxxNcvXQY+1p7V9Osmf5hiVJms1CjvTfAX6vqq4ErgNuT3IlsA94vKq2A4+3+wA3AtvbbS9wHwxeJIC7gGuBa4C7zrxQSJJWxryhX1Unq+q5tvxj4EVgM7AbONiaHQRuasu7gQdq4EngkiSbgBuAw1V1uqreBA4DO8c6GknSnBY1p59kG3AV8BRweVWdbKt+AFzeljcDrw9tdrzVRtXPfo69SaaSTM3MzCyme5KkeSw49JO8B/ga8Kmq+tHwuqoqoMbRoaraX1WTVTU5MTExjoeUJDULCv0kFzII/K9U1ddb+Y02bUP7earVTwBbhzbf0mqj6pKkFbKQq3cC3A+8WFWfH1p1CDhzBc4e4OGh+q3tKp7rgLfaNNBjwPVJNrYTuNe3miRphVywgDa/AXwc+G6SI632+8BngYeS3AZ8H/hYW/cosAuYBn4KfAKgqk4n+QzwTGv36ao6PZZRSJIWZN7Qr6pvAxmx+sOztC/g9hGPdQA4sJgOSpLGx0/kSlJHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR2ZN/STHEhyKsnzQ7W7k5xIcqTddg2tuzPJdJKXktwwVN/ZatNJ9o1/KJKk+SzkSP/LwM5Z6vdW1Y52exQgyZXAzcD72zZ/nGRDkg3AF4EbgSuBW1pbSdIKumC+BlX1rSTbFvh4u4EHq+pt4NUk08A1bd10Vb0CkOTB1vaFRfdYkrRk5zKnf0eSo236Z2OrbQZeH2pzvNVG1SVJK2ipoX8f8D5gB3AS+Ny4OpRkb5KpJFMzMzPjetg5bdv3CNv2PbIizyVJq2lJoV9Vb1TVu1X198CX+IcpnBPA1qGmW1ptVH22x95fVZNVNTkxMbGU7kmSRlhS6CfZNHT3o8CZK3sOATcnuTjJFcB24GngGWB7kiuSXMTgZO+hpXdbkrQU857ITfJV4IPAZUmOA3cBH0yyAyjgNeB3AarqWJKHGJygfQe4varebY9zB/AYsAE4UFXHxj4aSdKcFnL1zi2zlO+fo/09wD2z1B8FHl1U7yRJY+UnciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1JF5Qz/JgSSnkjw/VLs0yeEkL7efG1s9Sb6QZDrJ0SRXD22zp7V/Ocme5RmOJGkuCznS/zKw86zaPuDxqtoOPN7uA9wIbG+3vcB9MHiRAO4CrgWuAe4680IhSVo584Z+VX0LOH1WeTdwsC0fBG4aqj9QA08ClyTZBNwAHK6q01X1JnCYn30hkSQts6XO6V9eVSfb8g+Ay9vyZuD1oXbHW21U/Wck2ZtkKsnUzMzMErsnSZrNOZ/IraoCagx9OfN4+6tqsqomJyYmxvWwkiSWHvpvtGkb2s9TrX4C2DrUbkurjapLklbQUkP/EHDmCpw9wMND9VvbVTzXAW+1aaDHgOuTbGwncK9vNUnSCrpgvgZJvgp8ELgsyXEGV+F8FngoyW3A94GPteaPAruAaeCnwCcAqup0ks8Az7R2n66qs08OS5KW2byhX1W3jFj14VnaFnD7iMc5ABxYVO8kSWPlJ3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl+axbd8jq90FaWwMfUnqiKEvSR0x9CWpI92HvvO1knrSfehLUk8M/RF8B6Cznf074e+I1iJDXxrBUNd6ZOhLi7Bt3yO+GGhNM/SlOZwJeINe68W8/zH6XJK8BvwYeBd4p6omk1wK/CmwDXgN+FhVvZkkwH8HdgE/BX6nqp47l+eXloMBr/VsHEf6/6aqdlTVZLu/D3i8qrYDj7f7ADcC29ttL3DfGJ5bkrQIyzG9sxs42JYPAjcN1R+ogSeBS5JsWobnlySNcK6hX8BfJHk2yd5Wu7yqTrblHwCXt+XNwOtD2x5vtX8kyd4kU0mmZmZmzrF70uIsdGrHKSCtVec0pw98oKpOJPmnwOEkfz28sqoqSS3mAatqP7AfYHJyclHbSpLmdk5H+lV1ov08BXwDuAZ448y0Tft5qjU/AWwd2nxLq0mSVsiSQz/JLyZ575ll4HrgeeAQsKc12wM83JYPAbdm4DrgraFpIGnNcqpHa8m5TO9cDnxjcCUmFwD/s6r+V5JngIeS3AZ8H/hYa/8og8s1pxlcsvmJc3jusfCPVVJvlhz6VfUK8Guz1P8W+PAs9QJuX+rzSZLOnZ/IlZpzeefn1zNorTD0pXNg0GutMfQlqSOGviR15Fw/nCXpLLNN+bz22d9ahZ5IP8sjfYnxzc37NQ463xn60goYvrrHwNdqMvSlNc7LRbUYhr66Z2CuDv/dV4ehL62g4aA7c4Q+6kh9IaF49uOdL0b15XzqY6+8ekfdOt8CaDH92bbvkTmvCJpv/Uob7s/Z4zzf+rreGfrq0vkW+KMs5fLP2U4Yj9rmTJvFhO5CQ/rsx17quxmNV7eh7y+b1qNxXzI66kVhru0X01Yrr9vQl9aqpV76OVf7cR6VLyXkZ3v34LTP8vBErrrR83Xy5zrmlfg3O19PSq83GXzN/flpcnKypqamluWxF/JL5VHG+mCArG3+HS5ekmeranK2dR7pS1JHDH2tK7NdB6+1zX04Xl2eyF3KCSnfYi6vUSftFnt54NnLkv6xLkNf56f5wnquD/hofVvK5wk0uxWf3kmyM8lLSaaT7Fvp518qQ2ZxFhLgZ34upO1i2mv9ct+fuxW9eifJBuD/AB8BjgPPALdU1QuztR/31TvL8Qsz1ycdFzstsVaOYub7lOjwUZl/pFoJa+VvZ6XMdfXOSof+rwN3V9UN7f6dAFX1X2ZrP67QN3hmZyhLszvztzFqOnG5X2TO9YNp51Po/zaws6r+Xbv/ceDaqrpjqM1eYG+7+yvAS+fwlJcBf3MO268lPY0V+hqvY12/lmu8/7yqJmZbcd6dyK2q/cD+cTxWkqlRr3brTU9jhb7G61jXr9UY70qfyD0BbB26v6XVJEkrYKVD/xlge5IrklwE3AwcWuE+SFK3VnR6p6reSXIH8BiwAThQVceW8SnHMk20RvQ0VuhrvI51/Vrx8Z7XX7gmSRovv3tHkjpi6EtSR9Zl6K/Vr3pYjCSvJflukiNJplrt0iSHk7zcfm5c7X4uRZIDSU4leX6oNuvYMvCFtq+PJrl69Xq+NCPGe3eSE23/Hkmya2jdnW28LyW5YXV6vTRJtiZ5IskLSY4l+WSrr7v9O8dYV3ffVtW6ujE4Qfw94JeBi4DvAFeudr+WYZyvAZedVfuvwL62vA/4o9Xu5xLH9pvA1cDz840N2AX8ORDgOuCp1e7/mMZ7N/AfZml7Zfudvhi4ov2ub1jtMSxirJuAq9vyexl8LcuV63H/zjHWVd236/FI/xpguqpeqaq/Ax4Edq9yn1bKbuBgWz4I3LSKfVmyqvoWcPqs8qix7QYeqIEngUuSbFqZno7HiPGOsht4sKrerqpXgWkGv/NrQlWdrKrn2vKPgReBzazD/TvHWEdZkX27HkN/M/D60P3jzP0PvVYV8BdJnm1fXQFweVWdbMs/AC5fna4ti1FjW8/7+442pXFgaKpu3Yw3yTbgKuAp1vn+PWussIr7dj2Gfi8+UFVXAzcCtyf5zeGVNXi/uC6vx13PYxtyH/A+YAdwEvjc6nZnvJK8B/ga8Kmq+tHwuvW2f2cZ66ru2/UY+l181UNVnWg/TwHfYPA28I0zb33bz1Or18OxGzW2dbm/q+qNqnq3qv4e+BL/8DZ/zY83yYUMQvArVfX1Vl6X+3e2sa72vl2Pob/uv+ohyS8mee+ZZeB64HkG49zTmu0BHl6dHi6LUWM7BNzarvK4DnhraJpgzTpr3vqjDPYvDMZ7c5KLk1wBbAeeXun+LVWSAPcDL1bV54dWrbv9O2qsq75vV/sM9zKdNd/F4Ez594A/WO3+LMP4fpnBWf7vAMfOjBH4JeBx4GXgL4FLV7uvSxzfVxm87f1/DOY1bxs1NgZXdXyx7evvApOr3f8xjfdP2niOtjDYNNT+D9p4XwJuXO3+L3KsH2AwdXMUONJuu9bj/p1jrKu6b/0aBknqyHqc3pEkjWDoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI78f4wb8FzRdab1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "VideoCapture()\n",
    "eval_js('create()')\n",
    "\n",
    "CascadeFace = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "isFaceDetect=False\n",
    "margin = 30\n",
    "restr = 30\n",
    "n_images = 0\n",
    "Time = 1\n",
    "Picture_count = 1000\n",
    "while not isFaceDetect:\n",
    "  bigrect=[] \n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) # convert the image into GRAY\n",
    "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
    "  face = detect(gray, cascade = CascadeFace) # face contains the rectangle\n",
    " \n",
    "  if len(face)==0:\n",
    "    isFaceDetect=False\n",
    "  else:\n",
    "    isFaceDetect=True\n",
    "    \n",
    "  if isFaceDetect: # if a face is found\n",
    "    restricted_face = restrict(face[0])\n",
    "    hsv_roi=hsv[restricted_face[1]:restricted_face[3], restricted_face[0]:restricted_face[2]] # !!! better to restrict this face for the histogram\n",
    "    mask_roi = cv2.inRange(hsv_roi, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
    "    bigrect.append(compute_big_rect2(face[0])) # compute bigrect and add to the list\n",
    "  \n",
    "    isDetection=True # set a flag to start camshift\n",
    "    #gray_roi = gray[bigrect[0][1]:bigrect[0][3], bigrect[0][0]:bigrect[0][2]] # compute the region of interest\n",
    "    hist=cv2.calcHist([hsv_roi], [0], mask_roi, [180], [0,180]) # compute the histogram\n",
    "    hist=cv2.normalize(hist, hist,  alpha=0, beta=255, norm_type=cv2.NORM_MINMAX) # normalize the histogram \n",
    "    plt.hist(hsv_roi.ravel(),256,[0,256]) # show the histogram\n",
    "    plt.show()\n",
    "\n",
    "    track_window = (bigrect[0][0], bigrect[0][1], bigrect[0][2], bigrect[0][3]) # compute the region of interest (bigrect)\n",
    "\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 ) # sets some criteria for camshift\n",
    "\n",
    "while isDetection:\n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte)\n",
    "  hsv =  cv2.cvtColor(im, cv2.COLOR_RGB2HSV) #convert image to color\n",
    "  mask = cv2.inRange(hsv, np.array((0,64,32)), np.array((180,200,200)))\n",
    "  raw_im = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n",
    "  prob = cv2.calcBackProject([hsv], [0], hist, [0, 180], 1) # compute the probability matrix \n",
    "\n",
    "  for i in range(bigrect[0][0], bigrect[0][2]): # iterates between x1, x2\n",
    "    for j in range(bigrect[0][1], bigrect[0][3]): # iterates between y1, y2\n",
    "      prob[j,i]=0 # sets probability matrix to 0 on all points inside bigrect\n",
    "\n",
    "  prob &= mask\n",
    "\n",
    "  track_box, track_window = cv2.CamShift(prob, track_window, term_crit) # perform camshift over the track_window \n",
    "  im[:] = prob[...,np.newaxis] # add probability matrix to the image\n",
    "  cv2.ellipse(im, track_box, (255, 0, 0), 2) # draw the ellipse\n",
    "  \n",
    "  if n_images==Picture_count:\n",
    "    break\n",
    "  else:\n",
    "    time.sleep(Time)\n",
    "  #we crop the image to have only the hand\n",
    "  cropped_image = im[int(track_window[1]):int(track_window[1]+track_window[3]), int(track_window[0]):int(track_window[0]+track_window[2])]\n",
    "  \n",
    "  #save_image=input(\"Do you want to save the image Y or N: \")\n",
    "  resized_img = cv2.resize(cropped_image, (16, 16), interpolation = cv2.INTER_AREA)\n",
    "  resized_img = cv2.resize(resized_img, dsize=(1,256), interpolation=cv2.INTER_CUBIC)\n",
    "  resized_img = np.array([str(a[0][0]) for a in resized_img])\n",
    "  resized_img=resized_img.astype('float32')\n",
    "  resized_img /= 255.0\n",
    "  #print(resized_img[np.newaxis, :].shape)\n",
    "  prediction = loaded_model.predict(resized_img[np.newaxis, :]) # where hand_image is the probability image of your hand of size (1,256)\n",
    "  prediction = prediction.argmax()\n",
    "  predicted_letter = chr(ord('A') + prediction)\n",
    "\n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  cv2.putText(im, predicted_letter, (int(track_window[0]), int(track_window[1])), font, 1, (0, 255, 255), 2, cv2.LINE_4)\n",
    "\n",
    "  eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
    "  n_images+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBfK1Xa1_Hj2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPlD51Slb8akR4yHsG+hSk9",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
